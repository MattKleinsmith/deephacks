{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image generation\n",
    "\n",
    "In Part 1 of this course, we focused mainly on models that were useful for classification. However, many applications require generating much higher dimensional results, such as images and sentences. Examples include:\n",
    "* Text: neural translation, text to speech, image captioning\n",
    "* Image: Segmentation, artistic filters, image sharpening and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.misc import imsave\n",
    "from keras import metrics\n",
    "\n",
    "from vgg16_avg import VGG16_Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def limit_mem():\n",
    "    cfg = K.tf.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    K.set_session(K.tf.Session(config=cfg))\n",
    "limit_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_ITERATIONS=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first use case of an image to image architecture we're going to look at is neural style transfer, using the approach in [this paper](https://arxiv.org/abs/1508.06576). This is a fairly popular application of deep learning in which an image is recreated in the style of a work of art, such as Van Gogh's Starry Night. For more information about the use of neural networks in art, see this [Scientific American article](https://blogs.scientificamerican.com/sa-visual/neural-networks-for-artists/) or [Google's Magenta Project](https://magenta.tensorflow.org/welcome-to-magenta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "datapath = '/home/bfortuner/workplace/deephacks/'\n",
    "imgpath = datapath+'images/'\n",
    "stylepath = datapath+'styles/'\n",
    "resultspath = datapath+'results/'\n",
    "segmentations = imgpath+'segmentations/'\n",
    "IMG_EXTEN='.jpg'\n",
    "STYLE_FILENAME='mariana_s_portrait__avatar_style__by_kalisami-d9mhmho.png' #'starrynight.jpg'\n",
    "TEST_PHOTO_FILENAME='cat.jpg'#good_dog_img_for_testing.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save Filenames\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "fnames = list(glob.iglob(imgpath+'*'+IMG_EXTEN))\n",
    "pickle.dump(fnames, open(imgpath+'fnames.pkl', 'wb'))\n",
    "\n",
    "img_filenames = []\n",
    "img_filenames_w_path = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(imgpath):\n",
    "    img_filenames.extend(filenames)\n",
    "    img_filenames_w_path.extend(os.path.join(dirpath, f) \n",
    "                                for f in filenames if f.endswith(IMG_EXTEN))\n",
    "\n",
    "print (img_filenames[:5])\n",
    "print (img_filenames_w_path[:5])\n",
    "\n",
    "# Save filenames to file with pickle\n",
    "filenames_path = imgpath+\"fnamesfullpath.p\" \n",
    "pickle.dump(img_filenames_w_path, open(filenames_path, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load filenames\n",
    "fnames = pickle.load(open(filenames_path, 'rb'))\n",
    "N_files = len(fnames); N_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prepare Random Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = Image.open(fnames[random.randrange(N_files)]); \n",
    "img\n",
    "#img.crop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Specific Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = Image.open(imgpath+TEST_PHOTO_FILENAME) \n",
    "print(img.size)\n",
    "img "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to subtract the mean of each channel of the imagenet data and reverse the order of RGB->BGR since those are the preprocessing steps that the VGG authors did - so their model won't work unless we do the same thing.\n",
    "We can do this in one step using broadcasting, which is a topic we'll be returning to many times during this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rn_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "preproc = lambda x: (x - rn_mean)[:, :, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_arr = preproc(np.expand_dims(np.array(img), 0))\n",
    "img_shape = img_arr.shape\n",
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generate images from this network, we'll need to undo the above preprocessing in order to view them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deproc = lambda x,s: np.clip(x.reshape(s)[:, :, :, ::-1] + rn_mean, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_arr(arr): \n",
    "    plt.imshow(deproc(arr,arr.shape)[0].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_arr(img_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_img = Image.open(stylepath+STYLE_FILENAME)\n",
    "print (\"original size\" + str(style_img.size))\n",
    "\n",
    "style_img = style_img.resize(np.divide(style_img.size,3).astype('int32'));\n",
    "print (\"new size\" + str(style_img.size))\n",
    "style_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_arr = preproc(np.expand_dims(style_img,0)[:,:,:,:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize Images So They Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_w = style_arr.shape[1]\n",
    "style_h = style_arr.shape[2]\n",
    "img_w = img_arr.shape[1]\n",
    "img_h = img_arr.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_input = img_arr[:,:style_w,:style_h]\n",
    "style_input = style_arr[:,:img_w,:img_h]\n",
    "style_shape = style_input.shape\n",
    "img_shape = img_input.shape\n",
    "\n",
    "plot_arr(img_input)\n",
    "print(img_input.shape)\n",
    "\n",
    "# Our generated image should have the following shape\n",
    "output_img_shape = style_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_arr(style_input)\n",
    "print(style_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(img_shape)\n",
    "print(style_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Recreate Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first step in style transfer is understanding how to recreate an image from noise based on \"content loss\", which is the amount of difference between activations in some layer. In earlier layes, content loss is very similar to per-pixel loss, but in later layers it is capturing the \"meaning\" of a part of an image, rather than the specific details.\n",
    "\n",
    "To do this, we first take a CNN and pass an image through it. We then pass a \"noise image\" (i.e. random pixel values) through the same CNN. At some layer, we compare the outputs from it for both images. We then use a MSE to compare the activations of these two outputs. \n",
    "\n",
    "The interesting part is that now, instead of updating the parameters of the CNN, we update the pixels of the noisy image. In other words, our goal is to alter the noisy image so as to minimize the difference between the original image's output at some convolutional layer with the output of the noisy image at the same layer.\n",
    "\n",
    "In order to construct this architecture, we're going to be working with keras.backend, which is an abstraction layer that allows us to target both theano and tensorflow with the same code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The CNN we'll use is VGG16, but with a twist. Previously we've always used Vgg with max pooling, and this was useful for image classification. It's not as useful in this case however, because max pooling loses information about the original input area. Instead we will use average pooling, as this does not throw away as much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_model = VGG16_Avg(include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we're grabbing the activations from near the end of the convolutional model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_conv_layer = img_model.get_layer('block4_conv1').output\n",
    "img_conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And let's calculate the target activations for this layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "img_conv_layer_model = Model(img_model.input, img_conv_layer)\n",
    "img_activations = K.variable(img_conv_layer_model.predict(img_input))\n",
    "img_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In our implementation, we need to define an object that will allow us to separately access the loss function and gradients of a function, since that is what scikit-learn's optimizers require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    def __init__(self, f, shp): self.f, self.shp = f, shp\n",
    "        \n",
    "    def loss(self, x):\n",
    "        loss_, self.grad_values = self.f([x.reshape(self.shp)])\n",
    "        return loss_.astype(np.float64)\n",
    "\n",
    "    def grads(self, x): return self.grad_values.flatten().astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll define our loss function to calculate the mean squared error between the two outputs at the specified convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_loss = metrics.mse(img_conv_layer, img_activations)\n",
    "img_grads = K.gradients(img_loss, img_model.input)\n",
    "fn = K.function([img_model.input], [img_loss]+img_grads)\n",
    "evaluator = Evaluator(fn, img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we're going to optimize this loss function with a deterministic approach to optimization that uses a line search, which we can implement with sklearn's `fmin_l_bfgs_b` funtionc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "def solve_image(eval_obj, niter, x, shape):\n",
    "    for i in range(niter):\n",
    "        x, min_val, info = fmin_l_bfgs_b(eval_obj.loss, x.flatten(),\n",
    "                                         fprime=eval_obj.grads, maxfun=20)\n",
    "        x = np.clip(x, -127,127)\n",
    "        print('Current loss value:', min_val)\n",
    "        imsave(f'{resultspath}/res_at_iteration_{i}.png', deproc(x.copy(), shape)[0])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next we need to generate a random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def gen_rand_noise_img(shape):\n",
    "    return np.random.uniform(-2.5, 2.5, shape)\n",
    "\n",
    "rand_noise_img = gen_rand_noise_img(img_shape)\n",
    "plt.imshow(rand_noise_img[0]);\n",
    "assert img_shape == rand_noise_img.shape\n",
    "print (rand_noise_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we'll run through this optimization approach ten times and train the noise image's pixels as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iterations=DEFAULT_ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recreated_img = solve_image(evaluator, iterations, rand_noise_img, img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our result by comparing output at conv 1 of last block (5) is fairly amorphous, but still easily recognizable as a bird. Notice that the things it has reconstructed particularly well are those things that we expect Vgg16 to be good at recognizing, such as an eye or a beak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If instead we optimized by calculating loss from the output of conv 1 of 4th block, our trained image looks much more like the original. This makes sense because with less transformations to go through, comparing at an earlier layer means that we have a smaller receptive field and the features are more based on geometric details rather than broad features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Image.open(resultspath+'res_at_iteration_%s.png' % (iterations-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from matplotlib import animation, rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "def animate(i):\n",
    "    ax.imshow(Image.open(f'{resultspath}res_at_iteration_{i}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The optimizer first focuses on the important details of the bird, before trying to match the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(fig, animate, frames=DEFAULT_ITERATIONS, interval=200)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Recreate style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we've learned how to recreate an input image, we'll move onto attempting to recreate *style*. By \"style\", we mean the color palette and texture of an image. Unlike recreating based on content, with style we are not concerned about the actual structure of what we're creating, all we care about is that it captures this concept of \"style\".\n",
    "\n",
    "Here are some examples of images we can extract style from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We're going to repeat the same approach as before, but with some differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize VGG Model for Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "style_model = VGG16_Avg(include_top=False, input_shape=style_shape[1:])\n",
    "style_layer_outputs = {l.name: l.output for l in style_model.layers}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One thing to notice is that we're actually going to be calculating the loss function multiple layers, rather than just one. (Note however that there's no reason you couldn't try using multiple layers in your content loss function, if you wanted to try that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "style_layers = [style_layer_outputs['block{}_conv1'.format(o)] for o in range(1,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "style_layers_model = Model(style_model.input, style_layers)\n",
    "style_activations = [K.variable(o) for o in style_layers_model.predict(style_input)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The key difference is our choice of loss function. Whereas before we were calculating mse of the raw convolutional outputs, here we transform them into the \"gramian matrix\" of their channels (that is, the product of a matrix and its transpose) before taking their mse. It's unclear why this helps us achieve our goal, but it works. One thought is that the gramian  shows how our features at that convolutional layer correlate, and completely removes all location information. So matching the gram matrix of channels can only match some type of texture information, not location information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    # We want each row to be a channel, and the columns to be flattened x,y locations\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    # The dot product of this with its transpose shows the correlation \n",
    "    # between each pair of channels\n",
    "    return K.dot(features, K.transpose(features)) / x.get_shape().num_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_style_loss(x, targ): \n",
    "    return metrics.mse(gram_matrix(x), gram_matrix(targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "style_loss = sum(get_style_loss(l1[0], l2[0]) for l1,l2 in zip(style_layers, style_activations))\n",
    "style_grads = K.gradients(style_loss, style_model.input)\n",
    "style_fn = K.function([style_model.input], [style_loss]+style_grads)\n",
    "style_evaluator = Evaluator(style_fn, style_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We then solve as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iterations=DEFAULT_ITERATIONS\n",
    "rand_noise_img = gen_rand_noise_img(style_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "solve_image(style_evaluator, iterations, rand_noise_img, style_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our results are stunning. By transforming the convolutional outputs to the gramian, we are somehow able to update the noise pixels to produce an image that captures the raw style of the original image, with absolutely no structure or meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Image.open(resultspath+'res_at_iteration_%s.png' % (iterations-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now know how to reconstruct an image, as well as how to construct an image that captures the style of an original image. The obvious idea may be to just combine these two approaches by weighting and adding the two loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dimensions of image and style must match\n",
    "# So we need to do something about it\n",
    "print (img_shape)\n",
    "print (style_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Like before, we're going to grab a sequence of layer outputs to compute the style loss. However, we still only need one layer output to compute the content loss. How do we know which layer to grab? As we discussed earlier, the lower the layer, the more exact the content reconstruction will be. In merging content reconstruction with style, we might expect that a looser reconstruction of the content will allow more room for the style to have an effect (re: inspiration). Furthermore, a later layer ensures that the image \"looks like\" the same subject, even if it doesn't have the same details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "style_transfer_layers = [style_layer_outputs['block{}_conv1'.format(o)] for o in range(1,6)]\n",
    "content_transfer_layer_name = 'block4_conv2'\n",
    "content_transfer_layer = style_layer_outputs[content_transfer_layer_name]\n",
    "print(style_transfer_layers)\n",
    "print (content_transfer_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "style_transfer_model = Model(style_model.input, style_layers)\n",
    "style_transfer_activations = [K.variable(o) \n",
    "               for o in style_transfer_model.predict(style_input)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(img_input.shape)\n",
    "print (style_input.shape)\n",
    "print (img_model.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "content_transfer_model = Model(style_model.input, content_transfer_layer)\n",
    "content_transfer_activations = K.variable(content_transfer_model.predict(img_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now to actually merge the two approaches is as simple as merging their respective loss functions. Note that as opposed to our previous to functions, this function is producing three separate types of outputs: one for the original image, one for the image whose style we're emulating, and one for the random image whose pixel's we are training. \n",
    "\n",
    "One way for us to tune how the reconstructions mix is by changing the factor on the content loss, which we have here as 1/10. If we increase that denominator, the style will have a larger effect on the image, and if it's too large the original content of the image will be obscured by unstructured style. Likewise, if it is too small than the image will not have enough style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "style_transfer_loss = sum(get_style_loss(l1[0], l2[0]) for l1,l2 in zip(style_transfer_layers, style_transfer_activations))\n",
    "style_transfer_loss += metrics.mse(content_transfer_layer, content_transfer_activations)/10.\n",
    "style_transfer_grads = K.gradients(style_transfer_loss, style_model.input)\n",
    "style_transfer_fn = K.function([style_model.input], [style_transfer_loss]+style_transfer_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator(style_transfer_fn, output_img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iterations=DEFAULT_ITERATIONS\n",
    "rand_noise_img = gen_rand_noise_img(output_img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = solve_image(evaluator, iterations, rand_noise_img, output_img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These results are remarkable. Each does a fantastic job at recreating the original image in the style of the artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Image.open(resultspath+'res_at_iteration_%s.png' % (iterations-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are lots of interesting additional things you could try, such as the ideas shown here: https://github.com/titu1994/Neural-Style-Transfer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x = solve_image(evaluator, iterations, x, output_img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Image.open(resultspath+'res_at_iteration_%s.png' % (iterations-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
